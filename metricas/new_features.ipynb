{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "import scipy.stats as stats\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data configuration\n",
    "# data_dir = r'C:\\Users\\alail\\OneDrive\\Documentos\\dado_iniciais'\n",
    "data_dir = r'D:\\Pre-Processing Data'\n",
    "pen = [10, 25, 50, 75]\n",
    "\n",
    "columns_name = [\n",
    "    'File ID',\n",
    "    'Fault type',\n",
    "    'Fault resistance', \n",
    "    'Incidence angle',\n",
    "    'Fault location',\n",
    "    'Fault distance',\n",
    "]\n",
    "df_cases = pd.read_csv(\n",
    "    os.path.join(data_dir, 'cenarios_falta_preprocessamento.csv'),\n",
    "    header=None,\n",
    "    index_col=False,\n",
    "    names=columns_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b7c54da100438d81483a0c2cd77542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "if os.path.exists(os.path.join(data_dir, 'data.npy')) and False:\n",
    "    data = np.load(os.path.join(data_dir, 'data.npy'))\n",
    "else:\n",
    "    one_data_size = (6, 3072)\n",
    "    print(len(df_cases))\n",
    "    data = np.zeros((len(pen) * len(df_cases), *one_data_size))\n",
    "    for i, p in tqdm(enumerate(pen), total=len(pen)):\n",
    "        n = 0\n",
    "        for j, case in df_cases.iterrows():\n",
    "            data[i * len(df_cases) + n] = np.loadtxt(\n",
    "                os.path.join(data_dir, f'pen{p}', f'sinal_{j+1}_.csv'),\n",
    "                delimiter=','\n",
    "            ).T\n",
    "            n += 1\n",
    "    np.save(os.path.join(data_dir, 'data.npy'), data) # save in numpy for faster loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataframe to store the features\n",
    "columns_name = [\n",
    "    'File ID',\n",
    "    'Fault type',\n",
    "    'Fault resistance', \n",
    "    'Incidence angle',\n",
    "    'Fault location',\n",
    "    'Fault distance',\n",
    "    'Pen',\n",
    "]\n",
    "columns_type = {\n",
    "    'File ID': int,\n",
    "    'Fault type': int,\n",
    "    'Fault resistance': float,\n",
    "    'Incidence angle': float,\n",
    "    'Fault location': int,\n",
    "    'Fault distance': float,\n",
    "    'Pen': int\n",
    "}\n",
    "df_features = pd.DataFrame(\n",
    "    columns= columns_name,\n",
    "    index=range(len(pen) * len(df_cases))\n",
    "    \n",
    ")\n",
    "for i, p in enumerate(pen):\n",
    "    n = 0\n",
    "    for j, case in df_cases.iterrows():\n",
    "        df_features.loc[i * len(df_cases) + n] = case.tolist() + [p]\n",
    "        n += 1\n",
    "df_features = df_features.astype(columns_type)\n",
    "del df_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(data:np.ndarray, prefix:str='')->pd.DataFrame:\n",
    "    '''\n",
    "    parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        data to be processed\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    Energy : float\n",
    "        Energy of the signal\n",
    "    Entropy : float\n",
    "        Entropy of the signal\n",
    "    RMS Amplitude : float\n",
    "        RMS amplitude of the signal\n",
    "    Kurtose : float\n",
    "        Kurtosis of the signal\n",
    "    Skewness : float\n",
    "        Skewness of the signal\n",
    "    Singular Value Decomposition (SVD) : np.ndarray\n",
    "        Singular Value Decomposition of the signal\n",
    "    Mean : float\n",
    "        Mean of the signal\n",
    "    Median : float\n",
    "        Median of the signal\n",
    "    Standard Deviation : float\n",
    "        Standard deviation of the signal\n",
    "    Coefficient of Variation : float\n",
    "        Coefficient of variation of the signal\n",
    "    Minimum Value : float\n",
    "        Minimum value of the signal\n",
    "    Maximum Value : float\n",
    "        Maximum value of the signal\n",
    "    Variance : float\n",
    "        Variance of the signal\n",
    "    '''\n",
    "\n",
    "    # the return will be a dictionary\n",
    "    \n",
    "    res_dict = {\n",
    "        f'{prefix} Energy': np.sum(data**2, axis=-1),\n",
    "        f'{prefix} Entropy': stats.entropy(np.abs(data), axis=-1),\n",
    "        f'{prefix} RMS Amplitude': np.sqrt(np.mean(data**2, axis=-1)),\n",
    "        f'{prefix} Kurtosis': stats.kurtosis(data, axis=-1),\n",
    "        f'{prefix} Skewness': stats.skew(data, axis=-1),\n",
    "        f'{prefix} SVD': np.apply_along_axis(lambda x: svd(x[None, :])[1][0], -1, data),\n",
    "        f'{prefix} Mean': np.mean(data, axis=-1),\n",
    "        f'{prefix} Median': np.median(data, axis=-1),\n",
    "        f'{prefix} Standard Deviation': np.std(data, axis=-1),\n",
    "        f'{prefix} Coefficient of Variation': np.std(data, axis=-1) / np.mean(data, axis=-1),\n",
    "        f'{prefix} Minimum Value': np.min(data, axis=-1),\n",
    "        f'{prefix} Maximum Value': np.max(data, axis=-1),\n",
    "        f'{prefix} Variance': np.var(data, axis=-1),\n",
    "    }\n",
    "    return res_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 512*60\n",
    "smp_per_cycle = 512\n",
    "\n",
    "fault_idx = data.shape[-1]//2\n",
    "start_cycle1 = fault_idx - smp_per_cycle\n",
    "start_cycle2 = fault_idx - smp_per_cycle//2\n",
    "start_cycle3 = fault_idx\n",
    "names_prefix = ['Pre-', 'Fault-', 'Pos-']\n",
    "names_phase = ['Va', 'Vb', 'Vc', 'Ia', 'Ib', 'Ic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d52463dee3e406695d3339160d7d432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Fourier Transform\n",
    "df_fourier = df_features.copy()\n",
    "\n",
    "def get_fourier_feature(i):\n",
    "    fft_pre = np.abs(np.fft.fft(data[i,:,start_cycle1:start_cycle1+512], axis=-1))/smp_per_cycle\n",
    "    fft_fault = np.abs(np.fft.fft(data[i,:,start_cycle2:start_cycle2+512], axis=-1))/smp_per_cycle\n",
    "    fft_pos = np.abs(np.fft.fft(data[i,:,start_cycle3:start_cycle3+512], axis=-1))/smp_per_cycle\n",
    "    metrics = {}\n",
    "    for j in range(6):\n",
    "        metrics.update(get_metrics(fft_pre[j],   prefix=f'{names_prefix[0]}{names_phase[j]} - '))\n",
    "        metrics.update(get_metrics(fft_fault[j], prefix=f'{names_prefix[1]}{names_phase[j]} - '))\n",
    "        metrics.update(get_metrics(fft_pos[j],   prefix=f'{names_prefix[2]}{names_phase[j]} - '))\n",
    "    return metrics\n",
    "\n",
    "parallel = False\n",
    "metrics_all = []\n",
    "for i in tqdm(range(data.shape[0])):\n",
    "    metrics = get_fourier_feature(i)\n",
    "    metrics_all.append(metrics)\n",
    "    # if i == 0:\n",
    "    #     break\n",
    "df_fourier = df_fourier.join(pd.DataFrame(metrics_all))\n",
    "df_fourier.to_csv(\n",
    "    rf'new_features/fourier_features.csv', \n",
    "    index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavelet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa5ab0f86fe4002a4c82867984ceeb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pywt\n",
    "df_wavelet = df_features.copy()\n",
    "\n",
    "max_level = pywt.dwt_max_level(data[0,0,start_cycle1:start_cycle1+512].shape[-1], pywt.Wavelet('db4'))\n",
    "all_metrics = []\n",
    "for i in tqdm(range(data.shape[0])):\n",
    "    metrics = {}\n",
    "    for j in range(6):\n",
    "        coeffs = [\n",
    "            pywt.wavedec(data[i,j,start_cycle1:start_cycle1+512], 'db4', level=max_level),\n",
    "            pywt.wavedec(data[i,j,start_cycle2:start_cycle2+512], 'db4', level=max_level),\n",
    "            pywt.wavedec(data[i,j,start_cycle3:start_cycle3+512], 'db4', level=max_level)\n",
    "        ]\n",
    "        for k in range(0, max_level+1):\n",
    "            metrics.update(get_metrics(coeffs[0][k], prefix=f'Level {k}: {names_prefix[0]}{names_phase[j]} -'))\n",
    "            metrics.update(get_metrics(coeffs[1][k], prefix=f'Level {k}: {names_prefix[1]}{names_phase[j]} -'))\n",
    "            metrics.update(get_metrics(coeffs[2][k], prefix=f'Level {k}: {names_prefix[2]}{names_phase[j]} -'))      \n",
    "    all_metrics.append(metrics)\n",
    "    # if i == 1:\n",
    "    #     break\n",
    "df_wavelet = df_wavelet.join(pd.DataFrame(all_metrics))\n",
    "df_wavelet.to_csv(\n",
    "    rf'new_features/wavelet_features.csv', \n",
    "    index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stockwell Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c901499c1844140a70ddea3aac458e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alailton\\AppData\\Local\\Temp\\ipykernel_22680\\426882219.py:44: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  f'{prefix} Kurtosis': stats.kurtosis(data, axis=-1),\n",
      "C:\\Users\\Alailton\\AppData\\Local\\Temp\\ipykernel_22680\\426882219.py:45: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  f'{prefix} Skewness': stats.skew(data, axis=-1),\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(r'new_features.ipynb'))))\n",
    "from functions.TS.ST import stockwell_transform\n",
    "\n",
    "df_stockwell = df_features.copy()\n",
    "\n",
    "all_metrics = []\n",
    "for i in tqdm(range(data.shape[0])):\n",
    "    metrics = {}\n",
    "    for j in range(6):\n",
    "        matrixs = [\n",
    "            np.abs(stockwell_transform(data[i,j,start_cycle1:start_cycle1+512], 0.05)),\n",
    "            np.abs(stockwell_transform(data[i,j,start_cycle2:start_cycle2+512], 0.05)),\n",
    "            np.abs(stockwell_transform(data[i,j,start_cycle3:start_cycle3+512], 0.05))\n",
    "        ]\n",
    "        MAF = [\n",
    "            np.max(matrixs[0], axis=0),\n",
    "            np.max(matrixs[1], axis=0),\n",
    "            np.max(matrixs[2], axis=0)\n",
    "        ]\n",
    "        MAT = [\n",
    "            np.max(matrixs[0].T, axis=0),\n",
    "            np.max(matrixs[1].T, axis=0),\n",
    "            np.max(matrixs[2].T, axis=0)\n",
    "        ]\n",
    "        metrics.update(get_metrics(MAF[0], prefix=f'MAF: {names_prefix[0]}{names_phase[j]} -'))\n",
    "        metrics.update(get_metrics(MAF[1], prefix=f'MAF: {names_prefix[1]}{names_phase[j]} -'))\n",
    "        metrics.update(get_metrics(MAF[2], prefix=f'MAF: {names_prefix[2]}{names_phase[j]} -'))\n",
    "        metrics.update(get_metrics(MAT[0], prefix=f'MAT: {names_prefix[0]}{names_phase[j]} -'))\n",
    "        metrics.update(get_metrics(MAT[1], prefix=f'MAT: {names_prefix[1]}{names_phase[j]} -'))\n",
    "        metrics.update(get_metrics(MAT[2], prefix=f'MAT: {names_prefix[2]}{names_phase[j]} -'))\n",
    "    all_metrics.append(metrics)\n",
    "    # if i == 5:\n",
    "    #     break\n",
    "df_stockwell = df_stockwell.join(pd.DataFrame(all_metrics))\n",
    "df_stockwell.to_csv(\n",
    "    rf'new_features/stockwell_features.csv', \n",
    "    index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three-Phase Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a811179f1854130b71819e14a695fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Three-Phase Transform\n",
    "df_threePhase = df_features.copy()\n",
    "\n",
    "from functions.TT import clarke as clk\n",
    "from functions.TT import componentes_simetricas as cs\n",
    "\n",
    "sym_maxtix = (1/3)*np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, np.exp(2j*np.pi/3), np.exp(-2j*np.pi/3)],\n",
    "    [1, np.exp(-2j*np.pi/3), np.exp(2j*np.pi/3)],\n",
    "])\n",
    "clark_matrix = np.array([\n",
    "    [1/3, 1/3, 1/3], \n",
    "    [2/3, -1/3, -1/3], \n",
    "    [0, (3**0.5)/3, -(3**0.5)/3]\n",
    "])\n",
    "\n",
    "metrics_all = []\n",
    "for i in tqdm(range(data.shape[0])):\n",
    "    slides = np.lib.stride_tricks.sliding_window_view(data[i,:,start_cycle1:start_cycle3+512], window_shape=smp_per_cycle, axis=-1)\n",
    "    fft = np.fft.fft(slides, axis=-1)\n",
    "    fft = fft/smp_per_cycle\n",
    "\n",
    "    # Symetrical Components\n",
    "    v012 = np.abs(np.dot(sym_maxtix, fft[0:3,:,1]))\n",
    "    i012 = np.abs(np.dot(sym_maxtix, fft[3:6,:,1]))\n",
    "    # Parks Transform\n",
    "    v_clark = np.abs(np.dot(clark_matrix, fft[0:3,:,1]))\n",
    "    i_clark = np.abs(np.dot(clark_matrix, fft[3:6,:,1]))\n",
    "\n",
    "    sym_names = ['0', '1', '2']\n",
    "    clark_names = ['Alpha', 'Beta', 'Zero']\n",
    "\n",
    "    metrics = {}\n",
    "    for j in range(3):\n",
    "        metrics.update(get_metrics(v012[j], prefix=f'Sym: V{sym_names[j]} - '))\n",
    "        metrics.update(get_metrics(i012[j], prefix=f'Sym: I{sym_names[j]} - '))\n",
    "        metrics.update(get_metrics(v_clark[j], prefix=f'Clark: V_{clark_names[j]} - '))\n",
    "        metrics.update(get_metrics(i_clark[j], prefix=f'Clark: I_{clark_names[j]} - '))\n",
    "    metrics_all.append(metrics)\n",
    "    # break\n",
    "\n",
    "df_threePhase = df_threePhase.join(pd.DataFrame(metrics_all))\n",
    "df_threePhase.to_csv(\n",
    "    rf'new_features/threePhase_features.csv', \n",
    "    index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMD Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1dd281415b84d74b084d1fcb4b7393c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alailton\\AppData\\Local\\Temp\\ipykernel_22680\\426882219.py:50: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  f'{prefix} Coefficient of Variation': np.std(data, axis=-1) / np.mean(data, axis=-1),\n"
     ]
    }
   ],
   "source": [
    "import emd\n",
    "df_emd = df_features.copy()\n",
    "all_metrics = []\n",
    "for i in tqdm(range(data.shape[0])):\n",
    "    metrics = {}\n",
    "    for j in range(6):\n",
    "        imfs_pre = emd.sift.ensemble_sift(data[i,j,start_cycle1:start_cycle1+512], max_imfs=5).T\n",
    "        imfs_fault = emd.sift.ensemble_sift(data[i,j,start_cycle2:start_cycle2+512], max_imfs=5).T\n",
    "        imfs_pos = emd.sift.ensemble_sift(data[i,j,start_cycle3:start_cycle3+512], max_imfs=5).T\n",
    "\n",
    "        if imfs_pre.shape[0] < 5:\n",
    "            imfs_pre = np.pad(imfs_pre, ((0, 5-imfs_pre.shape[0]), (0, 0)), mode='constant')\n",
    "        if imfs_fault.shape[0] < 5:\n",
    "            imfs_fault = np.pad(imfs_fault, ((0, 5-imfs_fault.shape[0]), (0, 0)), mode='constant')\n",
    "        if imfs_pos.shape[0] < 5:\n",
    "            imfs_pos = np.pad(imfs_pos, ((0, 5-imfs_pos.shape[0]), (0, 0)), mode='constant')\n",
    "            \n",
    "        \n",
    "        for k in range(imfs_pre.shape[0]):\n",
    "            metrics.update(get_metrics(imfs_pre[k], prefix=f'IMF {k}: {names_prefix[0]}{names_phase[j]} -'))\n",
    "            metrics.update(get_metrics(imfs_fault[k], prefix=f'IMF {k}: {names_prefix[1]}{names_phase[j]} -'))\n",
    "            metrics.update(get_metrics(imfs_pos[k], prefix=f'IMF {k}: {names_prefix[2]}{names_phase[j]} -'))\n",
    "    all_metrics.append(metrics)\n",
    "    # if i == 1:\n",
    "    #     break\n",
    "df_emd = df_emd.join(pd.DataFrame(all_metrics))\n",
    "df_emd.to_csv(\n",
    "    rf'new_features/emd_features.csv', \n",
    "    index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VMD Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a48e60d0f064f939cabcda925689200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Metricas_VMD import apply_vmd\n",
    "df_vmd = df_features.copy()\n",
    "all_metrics = []\n",
    "for i in tqdm(range(data.shape[0])):\n",
    "    metrics = {}\n",
    "    for j in range(6):\n",
    "        imfs_pre = np.abs(apply_vmd(data[i,j,start_cycle1:start_cycle1+512]).T)\n",
    "        imfs_fault = np.abs(apply_vmd(data[i,j,start_cycle2:start_cycle2+512]).T)\n",
    "        imfs_pos = np.abs(apply_vmd(data[i,j,start_cycle3:start_cycle3+512]).T)\n",
    "        for k in range(imfs_pre.shape[0]):\n",
    "            metrics.update(get_metrics(imfs_pre[k], prefix=f'IMF {k}: {names_prefix[0]}{names_phase[j]} -'))\n",
    "            metrics.update(get_metrics(imfs_fault[k], prefix=f'IMF {k}: {names_prefix[1]}{names_phase[j]} -'))\n",
    "            metrics.update(get_metrics(imfs_pos[k], prefix=f'IMF {k}: {names_prefix[2]}{names_phase[j]} -'))\n",
    "    all_metrics.append(metrics)\n",
    "    # if i == 1:\n",
    "    #     break\n",
    "df_vmd = df_vmd.join(pd.DataFrame(all_metrics))\n",
    "df_vmd.to_csv(\n",
    "    rf'new_features/vmd_features.csv', \n",
    "    index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
